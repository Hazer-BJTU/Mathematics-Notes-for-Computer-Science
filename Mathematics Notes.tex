\documentclass[UTF8]{book}
\usepackage{ctex}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{lmodern}
\usepackage{amsmath,amsthm,amssymb,amsfonts}

\geometry{a4paper,scale=0.75} %设置页边距

\title{\Huge{Mathematics Notes} \\ \huge{for} \\ \Huge{Computer Science} \\ \Huge{Information Technology}}
\author{\Large Hazer-BJTU}
\date{\Large 2024 / 2 / 16}

\begin{document}
\maketitle
\tableofcontents %生成目录
\newpage

% \CTEXsetup[format={\Large\bfseries}]{section} %设置段落标题左对齐

\section{深度学习中的线性代数/概率论}

\subsection{多元函数微分}
考虑定义在$\mathbb{R}^n$上的函数$f$，其输出为一个向量$\mathbf{y}\in \mathbb{R}^m$，如果存在线性函数$L$，使得：
\begin{large}
    \begin{equation}
        \begin{aligned}
            f(\mathbf{x}+\mathbf{h})=f(\mathbf{x})+L(\mathbf{h})+O\left (\left \| \mathbf{h} \right \|_2\right )
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
其中线性函数$L$满足：
\begin{large}
    \begin{equation}
        \begin{aligned}
            &L(\mathbf{x}+\mathbf{y})=L(\mathbf{x})+L(\mathbf{y}) \\
            &L(\lambda \cdot \mathbf x)=\lambda \cdot L(\mathbf x), \lambda \in \mathbb{R}
            \nonumber 
        \end{aligned}
    \end{equation}
\end{large}
那么我们就认为该函数$f$是\textbf{可微的}，一般来说，我们可以将线性函数$L$简单理解为线性变换，如果我们限制函数$f$的输出为一个实数$y\in\mathbb{R}$，则微分也可以被表示为如下形式：
\begin{large}
    \begin{equation}
        \begin{aligned}
            f(\mathbf{x}+\mathbf{h})=f(\mathbf{x})+\mathbf{w}^\top\mathbf{h}+O\left (\left \| \mathbf{h} \right \|_2\right ), \mathbf{w}\in \mathbb{R}^n
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
一个基本的事实是可微$\Rightarrow$偏导数存在，因为：
\begin{large}
    \begin{equation}
        \begin{aligned}
            &\frac{f(\mathbf{x}+\mathbf{h}_i)-f(\mathbf{x})}{\Delta\mathbf{x}_i} = \frac{\mathbf{w}_i \cdot \Delta\mathbf{x}_i}{\Delta\mathbf{x}_i}+\frac{O(\Delta\mathbf{x}_i)}{\Delta\mathbf{x}_i}= \mathbf{w}_i+\frac{O(\Delta\mathbf{x}_i)}{\Delta\mathbf{x}_i} \\
            &\Rightarrow \lim_{\Delta\mathbf{x}_{i}\to 0} \frac{f(\mathbf{x}+\mathbf{h}_{i})-f(\mathbf{x})}{\Delta\mathbf{x}_i}=\mathbf{w}_i+\lim_{\Delta\mathbf{x}_{i}\to 0}\frac{O(\Delta\mathbf{x}_i)}{\Delta\mathbf{x}_i}=\mathbf{w}_i \\
            &\Rightarrow \frac{\partial f}{\partial \mathbf{x}_i}=\mathbf{w}_i 
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
由此可见，实际上向量$\mathbf{w}$就是由函数$f$关于各分量的偏导数构成的：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \mathbf{w}=\left ( \frac{\partial f}{\partial \mathbf{x}_1},\frac{\partial f}{\partial \mathbf{x}_2},\frac{\partial f}{\partial \mathbf{x}_3},\dots,\frac{\partial f}{\partial \mathbf{x}_n} \right )^\top
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
定义对于向量$\mathbf{x}\in\mathbb{R}^n$: $\mathrm{d}\mathbf{x}=(\mathrm{d}\mathbf{x}_1,\mathrm{d}\mathbf{x}_2,\mathrm{d}\mathbf{x}_3,\dots,\mathrm{d}\mathbf{x}_n)$，则根据全微分公式可以得出如下关系：
\begin{large}
    \begin{equation}
        \begin{aligned}
            &\mathrm{d}\mathbf{x}^\top\mathbf{x}=2\mathbf{x}^\top\mathrm{d}\mathbf{x} \\
            &\mathrm{d}(\mathbf{x}+\mathbf{y})=\mathrm{d}\mathbf{x}+\mathrm{d}\mathbf{y} \\
            &\mathrm{d}{\mathbf{A}\mathbf{x}}=\mathbf{A}\mathrm{d}\mathbf{x} \\
            &\mathrm{d}{\mathbf{x}^\top \mathbf{A} \mathbf{x}}=2\mathbf{x}^\top \mathbf{A} \mathrm{d}\mathbf{x}
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
在此只证明最后一条，注意到：
\begin{large}
    \begin{equation}
        \begin{aligned}
            &\mathbf{x}^\top \mathbf{A} \mathbf{x}=\sum_{i=1}^{n}\sum_{j=1}^{n}\mathbf{A}_{i,j}\mathbf{x}_i\mathbf{x}_j \\
            \frac{\partial}{\partial \mathbf{x}_i}&\mathbf{x}^\top \mathbf{A} \mathbf{x}=2\mathbf{A}_{i,i}\mathbf{x}_i+2\sum_{1 \le j \le n, j\not = i}{\mathbf{A}_{i,j}\mathbf{x}_j}=2\sum_{j=1}^{n}\mathbf{A}_{i,j}\mathbf{x}_j \\
            \Rightarrow \mathrm{d}&\mathbf{x}^\top \mathbf{A} \mathbf{x}=2\sum_{i=1}^{n}{\sum_{j=1}^{n}\mathbf{A}_{i,j}\mathbf{x}_j\mathrm{d}\mathbf{x}_i}=2\mathbf{x}^\top \mathbf{A}\mathrm{d}\mathbf{x}
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
与一元函数同理，如果上述函数$f$满足二阶偏导数连续的条件，则我们也可以利用Hessian矩阵做出更高阶的估计：
\begin{large}
    \begin{equation}
        \begin{aligned}
            f(\mathbf{x}+\mathbf{h})=f(\mathbf{x})+\mathbf{w}^\top \mathbf{h}+\frac{1}{2}\mathbf{h}^\top \mathbf{H}\mathbf{h}+O(\left \| \mathbf{h} \right \|_2^2)
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
其中Hessian矩阵的形式为：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \mathbf{H}_{i,j}=\frac{\partial^2f}{\partial\mathbf{x}_i\partial\mathbf{x}_j}
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
        

\subsection{线性回归模型的解析解}
一般的线性模型可以被描述为以下形式，其中 $\hat{y}\in \mathbb{R}, \mathbf{x}\in \mathbb{R}^d, \mathbf{w}\in \mathbb{R}^d$ :
\begin{large}
    \begin{equation}
        \begin{aligned}
            \hat{y}= \mathbf{w}^\top \mathbf{x}+\mathbf{b}
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
而对于批量的样本数据，使用 $\mathbf{X}\in \mathbb{R}^{n\times d}$ 表示 $n$ 组样本，$\hat{\mathbf{Y}}\in \mathbb{R}^{n}$ 表示对于数据集上所有样本的预测结果向量，则可以进行如下矩阵表示：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \hat{\mathbf{Y}}=\mathbf{X}\mathbf{w}+\mathbf{B}
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
对于真实的数据$Y$，线性回归要求我们最小化损失$\left \| \hat{\mathbf{Y}}-\mathbf{Y} \right \|_2$，这是一个十分简单的优化问题，存在解析解，证明如下：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \left \| \hat{\mathbf{Y}}-\mathbf{Y} \right \|_2 &= \sqrt{(\hat{\mathbf{Y}}-\mathbf{Y})^\top(\hat{\mathbf{Y}}-\mathbf{Y})} \\
            &= \sqrt{(\mathbf{X}\mathbf{w}+\mathbf{B}-\mathbf{Y})^\top(\mathbf{X}\mathbf{w}+\mathbf{B}-\mathbf{Y})}
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
故问题转化为最小化$(\mathbf{X}\mathbf{w}+\mathbf{B}-\mathbf{Y})^\top(\mathbf{X}\mathbf{w}+\mathbf{B}-\mathbf{Y})$，这是一个二次型，我们对于$\mathbf{w}$求导：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \mathrm{d}(\mathbf{X}\mathbf{w}+\mathbf{B}-\mathbf{Y})^\top(\mathbf{X}\mathbf{w}+\mathbf{B}-\mathbf{Y}) &= 2(\mathbf{X}\mathbf{w}+\mathbf{B}-\mathbf{Y})^\top\mathrm{d}(\mathbf{X}\mathbf{w}+\mathbf{B}-\mathbf{Y}) \\
            &= 2(\mathbf{X}\mathbf{w}+\mathbf{B}-\mathbf{Y})^\top\mathbf{X}\mathrm{d}\mathbf{w} \\
            &= 0
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
故可以得到：
\begin{large}
    \begin{equation}
        \begin{aligned}
            (\mathbf{X}\mathbf{w}+\mathbf{B}-\mathbf{Y})^\top\mathbf{X}=\mathbf{O}
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
等式两边同时取转置可知：
\begin{large}
    \begin{equation}
        \begin{aligned}
            &\mathbf{X}^\top(\mathbf{X}\mathbf{w}+\mathbf{B}-\mathbf{Y})=\mathbf{O} \\
            &\mathbf{X}^\top\mathbf{X}\mathbf{w}=\mathbf{X}^\top(\mathbf{Y}-\mathbf{B}) \\
            &\mathbf{w}=(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top(\mathbf{Y}-\mathbf{B})
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
即可得到参数的最优解，前提是矩阵$\mathbf{X}^\top\mathbf{X}$可逆。

\subsection{SVD奇异值分解}
一般来说，任何实矩阵$\mathbf{A}\in \mathbb{R}^{n\times m}$都可以被无条件地分解为如下三个矩阵的乘积：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \mathbf{A}_{n\times m}=\mathbf{U}_{n\times n}\mathbf{\Sigma}_{n\times m}\mathbf{V}_{m\times m}^\top
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
其中$\mathbf{U},\mathbf{V}$均为正交矩阵，并且$\mathbf{\Sigma}$满足：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \mathbf{\Sigma}_{i,j}=\begin{cases}
                \sqrt{\lambda_i} & i=j\\
                0 & i\not = j
               \end{cases} 
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
考虑$\mathbf{A}^\top \mathbf{A}$，这是一个实对称矩阵，故其一定可以被正交对角化，也即存在正交矩阵$\mathbf{V}$，使得：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \mathbf{A}^\top \mathbf{A}=\mathbf{V}\mathbf{\Lambda}\mathbf{V}^\top
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
其中：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \mathbf{\Lambda}_{m\times m}=\begin{bmatrix}
                \lambda_1 &  &  & \\
                 & \lambda_2 &  & \\
                 &  & \ddots  & \\
                 &  &  &\lambda_m
               \end{bmatrix}
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
考虑如下一组向量，我们断言它们之间是互相正交的：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \frac{\mathbf{A}\mathbf{v}_1}{\sqrt{\lambda_1}},\frac{\mathbf{A}\mathbf{v}_2}{\sqrt{\lambda_2}},\frac{\mathbf{A}\mathbf{v}_3}{\sqrt{\lambda_3}},\dots,\frac{\mathbf{A}\mathbf{v}_m}{\sqrt{\lambda_m}}
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
证明如下：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \frac{\mathbf{A}\mathbf{v}_i}{\sqrt{\lambda}_i} \cdot \frac{\mathbf{A}\mathbf{v}_j}{\sqrt{\lambda}_j} = \frac{\mathbf{v}_i^\top\mathbf{A}^\top\mathbf{A}\mathbf{v}_j}{\sqrt{\lambda_i\lambda_j}} 
            = \frac{\lambda_j\mathbf{v}_i^\top\mathbf{v}_j}{\sqrt{\lambda_i\lambda_j}} 
            = \begin{cases}
                1 & i=j \\
                0 & i\not = j
                \end{cases}
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
若$m\ge n$，考虑如下矩阵：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \mathbf{U}_{n\times n}=\left (  \frac{\mathbf{A}\mathbf{v}_1}{\sqrt{\lambda_1}},\frac{\mathbf{A}\mathbf{v}_2}{\sqrt{\lambda_2}},\frac{\mathbf{A}\mathbf{v}_3}{\sqrt{\lambda_3}},\dots,\frac{\mathbf{A}\mathbf{v}_n}{\sqrt{\lambda_n}}\right )
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
根据上述证明，$\mathbf{U}$是正交矩阵，并且满足：
\begin{large}
    \begin{equation}
        \begin{aligned}
            &\mathbf{U}\mathbf{\Sigma}=\mathbf{A}\mathbf{V} \\
            &\mathbf{A}=\mathbf{A}\mathbf{V}\mathbf{V}^\top=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
若$m<n$，我们可以反过来对$\mathbf{A}^\top$做奇异值分解，也可以得到相同的结果，奇异值分解告诉我们：任何线性变换都可以被分解为一次旋转(旋转、反射或其复合)，一次维度变换及拉伸，一次旋转的复合。除此之外，其还可以被用于求一般矩阵的“逆”：
\begin{large}
    \begin{equation}
        \begin{aligned}
            &\mathbf{A}=\mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top \\
            &\mathbf{A}^+=\mathbf{V}\mathbf{\Sigma}^+\mathbf{U}^\top
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
其中$\mathbf{\Sigma}^+$由将$\mathbf{\Sigma}$中非零元素取倒数后再转置得到。

\subsection{极大似然估计与最小化交叉熵损失}

\section{算法/基础数学}

\subsection{离散傅里叶变换DFT与快速傅里叶变换FFT}
对于数列$\left \{ a_n \right \}, \left \{ b_n \right \},0 \le n < N$，我们可以如下定义其离散卷积：
\begin{large}
    \begin{equation}
        \begin{aligned}
            &\left ( a*b \right )_k=\sum_{i=0}^{k}{a_ib_{k-i}} \\
            &0 \le k < N
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
我们记单位根\large{$e^{\frac{2k\pi i}{n}}=\omega_{n}^k$}，则可以如下定义其离散傅里叶变换及其逆变换：
\begin{large}
    \begin{equation}
        \begin{aligned}
            &DFT(a)_k=\sum_{t=0}^{N-1}{a_t \cdot \omega_{N}^{-kt}} \\
            &a_k=\frac{1}{N}\sum_{t=0}^{N-1}{DFT(a)_t \cdot \omega_{N}^{kt}}
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
其中逆变换的证明如下：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \frac{1}{N}\sum_{t=0}^{N-1}{DFT(a)_t \cdot \omega_{N}^{kt}} &= \frac{1}{N}\sum_{t=0}^{N-1}\left ( \sum_{u=0}^{N-1}a_u \cdot \omega_N^{-tu} \right ) \cdot \omega_{N}^{kt} \\
            &= \frac{1}{N}\sum_{t=0}^{N-1}\sum_{u=0}^{N-1}a_u \cdot \omega_N^{t(k-u)} \\
            &= \frac{1}{N}\sum_{u=0}^{N-1}\sum_{t=0}^{N-1}a_u \cdot \omega_N^{t(k-u)}
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
首先考虑如果$u=k$，则有下式成立：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \omega_N^{t(k-u)}&=\omega_N^{0}=1 \\
            \sum_{t=0}^{N-1}a_u \cdot \omega_N^{t(k-u)}&=Na_u=Na_k
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
然后考虑如果$u \not = k$，注意到$\omega_N^N=1$，则有下式成立：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \sum_{t=0}^{N-1}a_u \cdot \omega_N^{t(k-u)} &= a_u\sum_{t=0}^{N-1}\omega_N^{t(k-u)} \\
            &= a_u \cdot \frac{1-\omega_N^{N(k-u)}}{1-\omega_N^{k-u}} \\
            &= 0
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
故综上所述，逆变换得证：
\begin{large}
    \begin{equation}
        \begin{aligned}
            \frac{1}{N}\sum_{t=0}^{N-1}{DFT(a)_t \cdot \omega_{N}^{kt}}=\frac{1}{N} \cdot Na_k=a_k
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
接着我们引入卷积定理的离散形式：
\begin{large}
    \begin{equation}
        \begin{aligned}
            a*b=DFT^{-1}\left ( DFT(a) \odot DFT(b) \right )
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
为了证明上式，我们只需要证明：
\begin{large}
    \begin{equation}
        \begin{aligned}
            (a*b)_k &= DFT^{-1}\left ( DFT(a) \odot DFT(b) \right )_k \\
            &0 \le k < N
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}
利用定义展开右式，同理可证：
\begin{large}
    \begin{equation}
        \begin{aligned}
            DFT^{-1}\left ( DFT(a) \odot DFT(b) \right )_k &= \frac{1}{N}\sum_{t=0}^{N-1}\left ( DFT(a) \odot DFT(b) \right )_t \cdot \omega_N^{kt}\\
            &= \frac{1}{N}\sum_{t=0}^{N-1} DFT(a)_t \cdot DFT(b)_t \cdot \omega_N^{kt} \\
            &= \frac{1}{N}\sum_{t=0}^{N-1} \left ( \sum_{n=0}^{N-1}{a_n \cdot \omega_{N}^{-tn}} \right ) \cdot \left ( \sum_{m=0}^{N-1}{b_m \cdot \omega_{N}^{-tm}} \right ) \cdot \omega_N^{kt} \\
            &= \frac{1}{N}\sum_{t=0}^{N-1} \left ( \sum_{n=0}^{N-1}\sum_{m=0}^{N-1}a_nb_m \cdot \omega_N^{-t(n+m)} \right ) \cdot \omega_N^{kt} \\
            &= \frac{1}{N}\sum_{n=0}^{N-1}\sum_{m=0}^{N-1}\sum_{t=0}^{N-1}a_nb_m \cdot \omega_N^{t(k-n-m)} \\
            &= \frac{1}{N}\sum_{n=0}^{N-1}\sum_{m=0}^{N-1}\left [ n+m=k \right ]Na_nb_m \\
            &= \sum_{n=0}^{k}a_nb_{k-n} \\
            &= (a*b)_k
            \nonumber
        \end{aligned}
    \end{equation}
\end{large}

\end{document}